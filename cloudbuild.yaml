cd ~/mlops-churn-pipeline

cat > cloudbuild.yaml << 'EOF'
steps:
  # Stage 1: Install dependencies
  - name: 'python:3.9'
    id: 'install-deps'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸ“¦ Installing dependencies..."
        pip install --upgrade pip
        pip install -r requirements.txt
        echo "âœ… Done"

  # Stage 2: Validate components
  - name: 'python:3.9'
    id: 'validate'
    waitFor: ['install-deps']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸ” Validating components..."
        ls -la components/
        echo "âœ… Components validated"

  # Stage 3: Compile pipeline
  - name: 'python:3.9'
    id: 'compile'
    waitFor: ['validate']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸ“¦ Compiling pipeline..."
        pip install kfp
        cd pipelines
        python house_price_pipeline.py || python churn_pipeline.py
        ls -lh *.json
        echo "âœ… Compiled"

  # Stage 4: Submit to Vertex AI
  - name: 'python:3.9'
    id: 'submit'
    waitFor: ['compile']
    env:
      - 'PROJECT_ID=$PROJECT_ID'
      - 'REGION=$_REGION'
      - 'BUCKET_NAME=$_BUCKET_NAME'
      - 'SHORT_SHA=$SHORT_SHA'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸš€ Submitting pipeline to Vertex AI..."
        
        # Install Vertex AI SDK with pipeline support
        pip install "google-cloud-aiplatform[pipelines]"
        
        cat > submit_pipeline.py << 'PYEOF'
        from google.cloud import aiplatform
        import os
        import glob
        
        PROJECT_ID = os.environ['PROJECT_ID']
        REGION = os.environ['REGION']
        BUCKET_NAME = os.environ['BUCKET_NAME']
        SHORT_SHA = os.environ['SHORT_SHA']
        
        print(f"Project: {PROJECT_ID}")
        print(f"Region: {REGION}")
        print(f"Bucket: {BUCKET_NAME}")
        
        aiplatform.init(project=PROJECT_ID, location=REGION)
        
        # Find the compiled pipeline JSON
        pipeline_files = glob.glob('pipelines/*.json')
        if not pipeline_files:
            print("âŒ No pipeline JSON found!")
            exit(1)
        
        pipeline_path = pipeline_files[0]
        print(f"Using pipeline: {pipeline_path}")
        
        job = aiplatform.PipelineJob(
            display_name=f"mlops-cicd-{SHORT_SHA}",
            template_path=pipeline_path,
            pipeline_root=f"gs://{BUCKET_NAME}/pipeline_root",
            enable_caching=False
        )
        
        job.submit()
        print(f"âœ… Pipeline submitted!")
        print(f"ðŸ”— View at: https://console.cloud.google.com/vertex-ai/locations/{REGION}/pipelines/runs?project={PROJECT_ID}")
        PYEOF
        
        python submit_pipeline.py

  # Stage 5: Notify
  - name: 'bash'
    id: 'notify'
    waitFor: ['submit']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "=============================="
        echo "âœ… CI/CD Pipeline Complete!"
        echo "=============================="
        echo "Build: ${BUILD_ID}"
        echo "Commit: ${SHORT_SHA}"
        echo "Project: ${PROJECT_ID}"
        echo "Region: ${_REGION}"
        echo "=============================="

substitutions:
  _REGION: 'us-central1'
  _BUCKET_NAME: '${PROJECT_ID}-mlops'

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'
  substitution_option: 'ALLOW_LOOSE'

timeout: '1800s'
EOF

echo "âœ… cloudbuild.yaml updated with pipeline dependencies"
